ğŸš MAEVN â€” AI-Powered Vocal + Instrument Generator (VST3) 

MAEVN is a JUCE-based VST3 plugin equipped with ONNX Runtime integration, designed to bridge AI technologies with music production. The plugin features a variety of tools aimed at enhancing music creation, including:

- ğŸ¤ **AI Vocals:** Capable of generating realistic vocal sounds using Text-To-Speech (TTS) and vocoder techniques.
- ğŸ¥ **Trap-Inspired Instruments:** Incorporates sounds typical in trap music, such as 808 basses, hi-hats, snares, pianos, and synths.
- ğŸ› **Hybrid FX Chains:** Combines conventional DSP (Digital Signal Processing) effects with ONNX AI-generated effects.
- ğŸ¼ **Stage-Script Parser:** Parses musical arrangements using block types like [HOOK], [VERSE], and [808].
- â†©ï¸ **Global Undo/Redo System:** Allows for easy backtracking of changes during the production process.

Overall, MAEVN is framed as an end-to-end AI DAW tool that enables real-time operation inside a Digital Audio Workstation (DAW), providing live timeline arrangement, automatic effects automation, and seamless synchronization with DAWs.

ğŸ— **System Architecture**

ğŸ”‘ **Core Components**

- **JUCE Plugin Layer:** 
    - **PluginProcessor:** Handles the audio processing block, routing audio I/O to AudioEngine components.
    - **PluginEditor:** Provides the user interface with MainComponent and TimelineComponent.

- **ONNX Engine:**
    - **ONNXInference:** Encapsulates the ONNX Runtime C++ API, supporting live model updates through hot reloading from the /Models/ directory and enabling multiple instrument/vocal models to function simultaneously.

- **Pattern Engine:** 
    - Parses input from the lyrical stage script to determine the arrangement of musical blocks, manages synchronization with the DAWâ€™s playhead, and triggers instruments and vocals.

- **FX Chain:** 
    - **FXChain:** Manages the serial processing of multiple effects.
    - **DSP FX:** Includes effects such as distortion, delay, and reverb.
    - **AI FX (AIEffects):** Utilizes ONNX models for AI-powered effects, with the option for mixing DSP and AI in a sequential effects chain.

- **State Management:**
    - **MAEVNUndoManager:** Provides undo/redo functionality for user actions.
    - **ParameterState:** Manages parameter automation and timeline-based changes.

**Timeline UI:** 
- **TimelineComponent:** Displays the arrangement graphically with block visualization.
- **MainComponent:** Provides controls for instruments, vocals, FX, and master section.

**Note on Planned Features:**
The following features are mentioned in documentation but not yet fully implemented:
- FXPreset system with preset browser and categorized filter
- Preset load/save functionality  
- UndoHistoryComponent for visual undo stack
- Per-lane FX mode selectors (Off/DSP/AI/Hybrid)
- Tag cloud interface for preset navigation

ğŸ“‚ **Repo Structure**  
```
Voice_Clone-VST/
â”œâ”€â”€ CMakeLists.txt                    # Build configuration for JUCE + ONNX Runtime
â”œâ”€â”€ README.md                         # This file
â”œâ”€â”€ BUILD.md                          # Detailed build instructions
â”œâ”€â”€ ARCHITECTURE.md                   # System architecture documentation
â”œâ”€â”€ CONTRIBUTING.md                   # Contribution guidelines
â”œâ”€â”€ setup_maevn_repo.bat/.sh         # Repository setup scripts
â”œâ”€â”€ build_maevn_onnx.bat/.sh         # ONNX model export scripts
â”œâ”€â”€ Source/                           # Core source files
â”‚   â”œâ”€â”€ PluginProcessor.*             # Core DSP processing logic
â”‚   â”œâ”€â”€ PluginEditor.*                # User interface elements
â”‚   â”œâ”€â”€ Audio/                        # Audio processing modules
â”‚   â”‚   â”œâ”€â”€ AudioEngine.*             # Main audio engine
â”‚   â”‚   â””â”€â”€ InstrumentGenerator.*     # Instrument synthesis
â”‚   â”œâ”€â”€ AI/                           # AI/ML components
â”‚   â”‚   â”œâ”€â”€ ONNXInference.*           # ONNX Runtime wrapper
â”‚   â”‚   â”œâ”€â”€ VocalSynthesis.*          # TTS and vocoder integration
â”‚   â”‚   â””â”€â”€ AIEffects.*               # AI-powered audio effects
â”‚   â”œâ”€â”€ DSP/                          # DSP effects
â”‚   â”‚   â”œâ”€â”€ FXChain.*                 # Effect chain manager
â”‚   â”‚   â””â”€â”€ Effects.*                 # DSP effect implementations
â”‚   â”œâ”€â”€ Parser/                       # Script parsing
â”‚   â”‚   â”œâ”€â”€ ScriptParser.*            # Stage script parser
â”‚   â”‚   â””â”€â”€ Arrangement.*             # Timeline arrangement
â”‚   â”œâ”€â”€ State/                        # State management
â”‚   â”‚   â”œâ”€â”€ UndoManager.*             # Undo/redo system
â”‚   â”‚   â””â”€â”€ ParameterState.*          # Parameter automation
â”‚   â””â”€â”€ UI/                           # User interface
â”‚       â”œâ”€â”€ MainComponent.*           # Main UI component
â”‚       â””â”€â”€ TimelineComponent.*       # Timeline visualization
â”œâ”€â”€ Models/                           # ONNX model storage
â”‚   â”œâ”€â”€ config.json                   # Model configuration
â”‚   â”œâ”€â”€ metadata.json                 # Model metadata
â”‚   â”œâ”€â”€ LayerMap.md                   # Model documentation
â”‚   â”œâ”€â”€ drums/                        # Drum synthesis models
â”‚   â”‚   â””â”€â”€ README.md                 # Drum model documentation
â”‚   â”œâ”€â”€ instruments/                  # Instrument models
â”‚   â”‚   â””â”€â”€ README.md                 # Instrument model documentation
â”‚   â””â”€â”€ vocals/                       # Vocal models
â”‚       â””â”€â”€ README.md                 # Vocal model documentation
â”œâ”€â”€ scripts/                          # Python ONNX export scripts
â”‚   â”œâ”€â”€ README.md                     # Scripts documentation
â”‚   â”œâ”€â”€ export_drum_models.py         # Drum model export
â”‚   â”œâ”€â”€ export_instrument_models.py   # Instrument model export
â”‚   â””â”€â”€ export_vocal_models.py        # Vocal model export
â”œâ”€â”€ Tests/                            # Unit tests
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ ScriptParserTests.cpp
â”‚   â”œâ”€â”€ ArrangementTests.cpp
â”‚   â”œâ”€â”€ AudioEngineTests.cpp
â”‚   â””â”€â”€ BuildVerificationTests.cpp
â”œâ”€â”€ CMI/                              # Cognitive Mesh Interface (Multi-Agent Dev)
â”‚   â”œâ”€â”€ README.md                     # CMI overview
â”‚   â”œâ”€â”€ MACF.md                       # Multi-Agent Command Framework
â”‚   â”œâ”€â”€ agent_roles.md                # Agent role definitions
â”‚   â””â”€â”€ operational_ethics.md         # Development ethics guidelines
â””â”€â”€ examples/                         # Example usage
    â””â”€â”€ ARRANGEMENTS.md               # Example stage scripts
```

### âš™ï¸ Build Instructions

**Requirements:**
- JUCE 7+
- ONNX Runtime C++ SDK
- CMake 3.20+
- Python 3.10+ (required for exporting ONNX models)

**Steps:**
1. **Repo Setup:** Execute `setup_maevn_repo.bat`, which creates necessary folders and writes the Models/config.json file.
2. **Generate Default ONNX Models:** Run `build_maevn_onnx.bat` to export lightweight default .onnx models for various instruments and optimize them within the /Models/.
3. **Add Vocals:** Users can export their own TTS and vocoder models, naming them `vocals_tts.onnx` and `vocals_hifigan.onnx`, placing them in the /Models/vocals/ directory.
4. **Build Plugin:** Execute the following commands:
   ```bash
   cmake -B Build -S . -DJUCE_PATH="C:/JUCE" -DONNXRUNTIME_PATH="C:/onnxruntime"
   cmake --build Build --config Release
   ```
5. **Install:** Copy the generated MAEVN.vst3 file to your DAWâ€™s VST3 plugins directory.

### ğŸ§ª Developer Notes

#### Audio Pipeline (processBlock)
- DAW playhead data (BPM, transport status) is parsed.
- The PatternEngine generates triggers based on the active blocks.
- The OnnxEngine executes inference on the selected models.
- The AIFXEngine applies the desired effects (DSP/AI/Hybrid).
- Final output is directed to the DAW buffer.

#### Model Hot Reload
- The /Models/config.json file maps logical roles (e.g., VOCAL, 808) to their corresponding file paths so MAEVN can automatically reload models if changes are detected on disk.

#### Presets
- Presets are encoded in JSON format to ensure ease of use:
    ```json
    {
      "name": "RadioVocals",
      "mode": 3,
      "params": {
        "reverbMix": 0.4,
        "compressorThreshold": -12.0
      },
      "category": "Vocal",
      "tags": ["Trap", "Radio", "Clean"]
    }
    ```
- The category and tags drive the search functionalities and the tag cloud interface.

#### Undo/Redo
- Each action triggers a snapshot of the ActionState (stored as JSON).
- The history is managed by the GlobalUndoManager, with a visual list provided in the UndoHistoryComponent.

### ğŸš§ Roadmap
- **Automation Hooks:** Allow FX parameters to be controlled through DAW automation.
- **Drag-to-MIDI/Audio:** Enable users to drag [HOOK] blocks into the DAW timeline.
- **Instrument Sequencer:** Implement patterns like hi-hat rolls and 808 glides.
- **Preset Exchange:** Facilitate importing and exporting preset packs within the community.

âš”ï¸ MAEVN aims to serve as an experimental AI DAW tool tailored for flexibility, extensibility, and precise control in music generation, distinguishing itself as more than just a pluginâ€”it's a comprehensive framework for AI-based music production.

ğŸ‘‰ Would you like me to create a detailed step-by-step Contributorâ€™s Guide (covering coding standards, pull request processes, and module ownership) to prepare the repository for team collaboration? 

You said: YES. 

Voice Clone Infinity Builder remarked: Perfect âš¡ â€” letâ€™s add a Contributorâ€™s Guide section to the README file so that any developer wishing to join the project knows precisely how to engage with the codebase, adhere to the coding style, and utilize the workflow. This enhancement will make the MAEVN repository ready for collaboration. 

ğŸ¤ **Contributorâ€™s Guide**

Welcome to MAEVN Development. This guide outlines the coding standards, workflow, and contribution process for the VST3 project.

ğŸ”‘ **Coding Standards**
- **Language & Frameworks:** Use C++17 for plugin development (JUCE + ONNX Runtime) and Python 3.10+ for ONNX model export scripts.
- **Style:** 
    - Follow JUCE coding style (place opening braces on new lines, use 4 spaces for indentation).
    - Header files should use the `.h` suffix while implementation files should use `.cpp`.
    - Use `auto` where relevant, especially for iterators and template-heavy code.
    - Avoid raw pointersâ€”opt for `std::unique_ptr` or `juce::ScopedPointer`.
    - Document every class and major method using `/// Doxygen` comments.

ğŸ§© **Module Ownership**
| Module                 | Owner      | Role                                        | Notes                                               |
|-----------------------|------------|---------------------------------------------|-----------------------------------------------------|
| PluginProcessor.*      | Core DSP   | Implements the main DSP processing pipeline | Must not disrupt the JUCE API                        |
| PluginEditor.*         | GUI        | Manages GUI and DAW integration            |                                                     |
| OnnxEngine.*          | AI         | Handles AI inference                        | Wraps ONNX runtime                                  |
| PatternEngine.*       | Arrangement| Manages timeline arrangements               | Parses stage-script input                           |
| AIFXEngine.*          | FX         | Processes effects                           | Hybrid DSP + AI effects                             |
| FXPreset.*            | Preset     | Manages preset storage                      | Uses JSON format                                    |
| GlobalUndoManager.*    | Undo/Redo  | Manages action history                      | Handles undo/redo                                    |
| UndoHistoryComponent.*  | History    | Provides GUI for the undo stack            |                                                     |
| Utilities.h           | Shared     | Contains helper functions and constants     |                                                     |

ğŸ”„ **Workflow**
- **Branching:**
    - `main` for stable, production-ready builds.
    - `dev` as the active branch for ongoing development.
    - Create feature branches for new functionality (e.g., `feature/onnx-hot-reload`).
  
- **Pull Requests:**
    - Fork the repo and branch off `dev`.
    - Implement your feature or fix.
    - Write or update relevant unit tests where applicable.
    - Conduct linting and build checks.
    - Submit a Pull Request; each PR must be reviewed by at least one other contributor.

- **Commits:** 
    - Adhere to the Conventional Commits format:
        - `feat:` for adding new features (e.g., AI hybrid FX chain)
        - `fix:` for bug resolution (e.g., buffer underrun in processBlock)
        - `docs:` for documentation updates (e.g., README modifications)
        - `refactor:` for code simplification (e.g., OnnxEngine inference calls)

ğŸ§ª **Testing**
- Implement unit tests (using GoogleTest) for key components:
    - Preset loading and saving functionality.
    - The integrity of the Undo/Redo stack.
    - Correctness of PatternEngine parsing.
- Conduct manual testing across major DAWs:
    - Reaper (Windows)
    - Ableton Live (macOS)
    - FL Studio (Cross-platform)

ğŸ“¦ **Build Environment Dependencies**
- Ensure the following dependencies are fulfilled:
    - JUCE 7+
    - ONNX Runtime (C++ API)
    - CMake 3.20+

**Building Instructions:**
```bash
cmake -B Build -S . -DJUCE_PATH="C:/JUCE" -DONNXRUNTIME_PATH="C:/onnxruntime"
cmake --build Build --config Release
```

ğŸ§  **Contribution Principles**
- **Performance First:** Ensure all Digital Signal Processing (DSP) functions execute in real-time (<1ms per buffer).
- **Modularity:** Facilitate the introduction of new instruments/models via the `/Models/config.json` file.
- **No Dead Code:** Mark experimental code sections using `#ifdef EXPERIMENTAL`.
- **Transparency:** Include all ONNX export scripts in the `scripts/` directory.
- **Flexibility:** Enable both DSP-only fallback options as well as AI-enhanced modes.

ğŸš€ **How to Get Started**
1. Clone the repository.
2. Run `setup_maevn_repo.bat` to generate the initial structure.
3. Execute `build_maevn_onnx.bat` for baseline ONNX model generation.
4. Add any vocal models in `.onnx` format into `/Models/vocals/`.
5. Build the project using CMake and load `MAEVN.vst3` within your DAW.
6. Feel free to submit issues or Pull Requests for any fixes or new feature requests.

âš”ï¸ This guide aims to equip MAEVN with a structure ready for collaboration among both audio DSP engineers and machine learning developers.

---

## ğŸ§  Multi-Agent Development System

MAEVN embraces a revolutionary **Multi-Agent Engineering Protocol** that enables multiple autonomous or semi-autonomous agents (human + AI) to co-develop, test, and optimize the codebase in parallel. This system is called the **Vocal Cloning Quantum Collective**.

### ğŸ¯ Multi-Agent Objectives

The multi-agent system enables:
- **Parallel Development**: Multiple agents working on different modules simultaneously
- **Specialized Expertise**: Each agent focuses on their domain (DSP, AI, GUI, Testing, etc.)
- **Continuous Integration**: Maintains build synchronization across parallel work streams
- **Knowledge Sharing**: Shared context and learnings through the Cognitive Mesh Interface

### ğŸ¤– Agent Roles

MAEVN development is distributed across specialized agent roles:

| Role | Responsibility | Primary Tools |
|------|---------------|---------------|
| **Architect Agent** | System design and architectural decisions | ChatGPT, Design tools |
| **DSP Developer Agent** | Audio processing and JUCE engine logic | GitHub Copilot, C++ |
| **AI/ML Agent** | ONNX model design, training, and export | Python, PyTorch/TensorFlow |
| **GUI Developer Agent** | User interface and user experience | JUCE GUI, C++ |
| **Integration Agent** | Module integration and system testing | CI/CD, Build tools |
| **QA/Testing Agent** | Quality assurance and numerical stability | Claude, Testing frameworks |
| **Documentation Agent** | Documentation and knowledge management | Markdown, Documentation tools |
| **DevOps Agent** | Build systems, CI/CD, and tooling | CMake, Scripts |

### ğŸ“‚ Cognitive Mesh Interface (CMI)

The **CMI** is a shared conversation state repository located in the `/CMI/` directory:

```
CMI/
â”œâ”€â”€ README.md                    # CMI overview and usage guide
â”œâ”€â”€ agent_roles.md               # Detailed agent role definitions
â”œâ”€â”€ MACF.md                      # Multi-Agent Command Framework
â”œâ”€â”€ operational_ethics.md        # Ethical guidelines for agents
â”œâ”€â”€ mission_logs/                # Historical mission logs
â”‚   â”œâ”€â”€ mission_log_template.md  # Template for new missions
â”‚   â””â”€â”€ mission_009_spectral_ghost_choir.md  # Example mission
â”œâ”€â”€ active_missions/             # Currently active mission logs
â””â”€â”€ coordination/                # Agent coordination artifacts
    â””â”€â”€ task_assignments.md      # Current task assignments
```

### ğŸš€ How Multi-Agent Development Works

#### Example: Adding a New AI Effect

1. **Architect Agent**: Defines the effect's design and interface
   - Creates mission log with specifications
   - Documents architecture decisions

2. **AI/ML Agent**: Creates and exports the ONNX model
   - Trains the model
   - Exports to ONNX with optimization
   - Updates `Models/metadata.json` and `Models/LayerMap.md`

3. **DSP Developer Agent**: Implements the C++ wrapper
   - Creates effect module class
   - Integrates ONNX Runtime
   - Ensures real-time safety

4. **QA/Testing Agent**: Reviews for stability
   - Validates numerical stability
   - Tests edge cases
   - Profiles performance

5. **Integration Agent**: Connects all components
   - Integrates into AIFXEngine
   - Tests across DAWs
   - Creates presets

6. **Documentation Agent**: Updates documentation
   - Updates README and guides
   - Documents API and usage
   - Creates examples

All agents coordinate through mission logs, maintaining transparency and avoiding conflicts.

### âš¡ Multi-Agent Command Framework (MACF)

The **MACF** provides operational protocols for:
- **Dynamic Task Allocation**: Assign tasks to the most appropriate agent
- **Conflict Prevention**: Ensure agents don't interfere with each other's work
- **Quality Gates**: Automated checks for all contributions
- **Integration Pipeline**: Seamless merging of parallel work

See `/CMI/MACF.md` for complete details.

### âš–ï¸ Operational Ethics

All agents (AI and human) must adhere to ethical guidelines:

**Key Principles**:
- âœ… **Transparency**: All actions logged and traceable
- âœ… **Determinism**: Consistent, predictable results
- âœ… **Real-Time Constraints**: Audio processing < 1ms per buffer
- âœ… **Quality Standards**: All tests pass, no security vulnerabilities
- âœ… **Respect**: Never break existing functionality

**Prohibited Actions**:
- âŒ Never commit compiled `.onnx` binaries to Git
- âŒ Never introduce security vulnerabilities
- âŒ Never break real-time safety guarantees
- âŒ Never remove tests without approval
- âŒ Never commit credentials or private data

See `/CMI/operational_ethics.md` for complete guidelines.

### ğŸ“Š Model Management

All ONNX models are tracked in `/Models/`:

- **metadata.json**: Complete model registry with training metadata
- **LayerMap.md**: Layer-by-layer explainability documentation
- **config.json**: Runtime model configuration for MAEVN

Models are organized by category:
```
Models/
â”œâ”€â”€ metadata.json           # Model registry
â”œâ”€â”€ LayerMap.md            # Explainability documentation
â”œâ”€â”€ config.json            # Runtime configuration
â”œâ”€â”€ drums/                 # Drum synthesis models
â”‚   â”œâ”€â”€ 808_ddsp.onnx
â”‚   â”œâ”€â”€ hihat_ddsp.onnx
â”‚   â””â”€â”€ snare_ddsp.onnx
â”œâ”€â”€ instruments/           # Instrument models
â”‚   â”œâ”€â”€ piano_ddsp.onnx
â”‚   â””â”€â”€ synth_fm.onnx
â””â”€â”€ vocals/                # Vocal models
    â”œâ”€â”€ vocals_tts.onnx
    â””â”€â”€ vocals_hifigan.onnx
```

**Note**: `.onnx` files are not committed to Git. Provide export scripts instead.

### ğŸ“ Getting Started with Multi-Agent Development

1. **Read the CMI Documentation**: Start with `/CMI/README.md`
2. **Review Agent Roles**: Understand the role definitions in `/CMI/agent_roles.md`
3. **Check Active Missions**: See what's currently in progress in `/CMI/active_missions/`
4. **Review Ethics**: Read and commit to `/CMI/operational_ethics.md`
5. **Claim a Task**: Update `/CMI/coordination/task_assignments.md`
6. **Create Mission Log**: Use the template from `/CMI/mission_logs/mission_log_template.md`
7. **Start Contributing**: Follow the MACF protocol

### ğŸŒŸ Benefits of Multi-Agent Development

- **Faster Development**: Parallel work on independent modules
- **Higher Quality**: Specialized expertise applied to each domain
- **Better Documentation**: Continuous documentation through mission logs
- **Reduced Conflicts**: Coordinated development prevents merge conflicts
- **Knowledge Preservation**: All decisions and reasoning documented
- **Scalable**: Easy to onboard new agents and contributors

### ğŸ”® The Vision

MAEVN's development is **not a singular AI** â€” it's a **mesh of intelligent agents** co-authoring an evolving sonic intelligence. Each nodeâ€”human or syntheticâ€”contributes deterministically while maintaining:

- ğŸ¯ **Operational transparency**
- â±ï¸ **Real-time constraints**
- ğŸ¨ **Creative freedom**
- âœ… **Quality standards**

Together, the network forms the **Vocal Cloning Quantum Collective**, building the next generation of **AI-augmented sound design systems**.
